{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#dome-recommendations-for-supervised-ml-validation-in-biology","title":"DOME Recommendations for supervised ML validation in biology","text":"<p>Motivation</p> <p> Modern biology frequently relies on machine learning (ML) to provide predictions and improve decision processes. There have been recent calls for more scrutiny on ML performance and possible limitations. </p> <p>What is DOME-ML?</p> <p> DOME-ML (or simply DOME) is an acronym standing for Data, Optimization, Model and Evaluation in ML. DOME is a set of community-wide guidelines, recommendations and checklists spanning these four areas aiming to help establish standards of supervised ML validation in biology. The recommendations are formulated as questions to anyone wishing to pursue implementation of a ML algorithm. Answers to these questions can be easily included in the supplementary material of published papers. </p> <p>What is the scope of the recommendations?</p> <p> The recommendations cover four separate aspects covering the major areas of ML: </p> <ul> <li>Data</li> <li>Optimization</li> <li>Model</li> <li>Evaluation</li> </ul> <p>About this training course</p> <p> Learning and using the DOME recommendations will increase awareness among researchers on best practices to share ML approaches  so they include minimum metadata for other researchers to get a quick and clear picture of the their ML approach is about and  how it compares to others. This course aims to be a useful and straightforward guide for those who wish to incorporate DOME in their work. </p>"},{"location":"#authors","title":"Authors","text":"<p>Fotis E. Psomopoulos</p> <p>  Senior Researcher(INAB|CERTH) </p> <p>Styliani-Christina Fragkouli</p> <p>  PhD Candidate (INAB|CERTH) </p>"},{"location":"#lesson-overview","title":"Lesson overview","text":"<p> Description The DOME recommendations are thoroughly presented. Every section is well described in order for all learners to be able to implement them.</p> <p></p> <p> Prerequisites To be able to follow this course, learners should have knowledge in: \u20031. ML basic concepts \u20032. Second requirement  </p> <p></p> <p> Learning Outcomes: By the end of the course, learners will be able to: \u20031. Recognize the DOME recommendations \u20032. Implement the DOME recommendations   </p> <p></p> <p> Target Audience: Anyone who uses ML in their research. </p> <p> Level: Beginner </p> <p> License: Creative Commons Attribution 4.0 International License </p> <p> Funding: This project has received funding from [name of funders].  </p>"},{"location":"#contributors","title":"Contributors","text":"<p>Artemis</p> <p>  Lorem ipsum dolor sit amet. </p> <p>Ares</p> <p>  Lorem ipsum dolor sit amet. </p> <p>Nike</p> <p>  Lorem ipsum dolor sit amet. </p> <p>Prometheus</p> <p>  Lorem ipsum dolor sit amet. </p> <p>Perseus</p> <p>  Lorem ipsum dolor sit amet. </p> <p>Hercules</p> <p>  Lorem ipsum dolor sit amet. </p>"},{"location":"#citing-this-lesson","title":"Citing this lesson","text":"<p>Please cite as:</p> <ol> <li>Enter your citation here.</li> <li>Geert van Geest, Elin Kronander, Jose Alejandro Romero Herrera, Nadja \u017dlender, &amp; Alexia Cardona. (2023). The ELIXIR Training Lesson Template - Developing Training Together (v1.0.0-alpha). Zenodo. https://doi.org/10.5281/zenodo.7913092. </li> </ol>"},{"location":"#setup","title":"Setup","text":""},{"location":"#software-setup","title":"Software setup","text":"<p>To run this course you just need to have an internet browser.</p>"},{"location":"course_schedule/","title":"Course schedule","text":"start end topic 10:00 10:30 coffee! 12:00 13:00 lunch! <p>Generate markdown tables at tablesgenerator.com</p>"},{"location":"follow_up_training/","title":"Follow up training","text":"<p>lorem ipsum</p>"},{"location":"chapters/bring-your-data/","title":"Bring your Data","text":"<p> Time to practice on your work or a publication of your choice! </p> <p>Steps:</p> <ol> <li>Read the publication you have chosen.</li> <li>Locate/highlight the areas of interest according to the DOME sections. (how many are covered? which are missing?)</li> <li>Fill in this (some kind of ready table template to be given) with the information.</li> <li>Discuss your results with the rest of the group. (do you see any common trends?)</li> <li>Grab a beverage \u2615 or a snack \ud83c\udf69 to celebrate!</li> </ol>"},{"location":"chapters/chapter_01/","title":"1. Data","text":"<p> ML models analyze experimental biological data by identifying patterns, which can then be used to generate biological insights from similar, previously unseen data.  The ability of a model to maintain its performance on new data is referred to as its generalization power.  Achieving strong generalization is a key challenge in developing ML models; without it, trained models cannot be effectively reused.  Properly preprocessing data and using it in an informed way are essential steps to ensure good generalization. </p> Further Reading <p>State-of-the-art ML models are often capable of memorizing all variations within the training data. As a result, when evaluated on data from the training set, they may give the false impression of excelling at the given task. However, their performance tends to diminish when tested on independent data (called the test or validation set), revealing a lower generalization power. To address this issue, the original dataset should be randomly split into non-overlapping parts. The simplest method involves creating separate training and test sets (with a possible third validation set). Alternatively, more robust techniques like cross-validation or bootstrapping, which repeatedly create different training/testing splits from the available data, are often preferred.</p> <p>Handling overlap between training and test data can be especially challenging in biology. For instance, in predicting entire gene or protein sequences, ensuring data independence might require reducing homologs in the dataset. In modeling enhancer\u2013promoter contacts, a different criterion may be needed, such as ensuring that no endpoint is shared between training and test sets. Similarly, modeling protein domains may require splitting multidomain sequences into their individual domains before applying homology reduction. Each biological field has its own methods for managing overlapping data, making it crucial to consult prior literature when developing an approach.</p> <p>Providing details on the size of the dataset and the distribution of data types helps demonstrate whether the data is well-represented across different sets. Simple visualizations, such as plots or tables, showing the number of classes (for classification), a histogram of binned values (for regression), and the various types of biological molecules included in the data, are essential for understanding each set. Additionally, for classification tasks, methods that account for imbalanced classes should be used if the class frequencies suggest a significant imbalance.</p> <p>It\u2019s also important to note that models trained on one dataset may not perform well on closely related, but distinct, datasets\u2014a phenomenon known as \u201ccovariance shift.\u201d This issue has been observed in several recent studies, such as those predicting disease risk from exome sequencing data. While covariance shift remains an open problem, potential solutions have been proposed, particularly in the field of transfer learning. Furthermore, building ML models that generalize well on small training datasets often requires specialized models and algorithms.</p> <p>Finally, making experimental data publicly available is crucial. Open access to datasets, including precise data splits, enhances the reproducibility of research and improves the overall quality of ML publications. If public repositories are not available, authors should be encouraged to use platforms like ELIXIR deposition databases or Zenodo to ensure long-term data accessibility.<sup>1</sup></p>"},{"location":"chapters/chapter_01/#11-provenance","title":"1.1 Provenance","text":"<p> Provenance of data refers to the origin, history, and lineage of data\u2014essentially, tracking where the data came from, how it has been processed, and how it has moved through various systems. It\u2019s like a detailed record that traces the data's life cycle from creation to its current state. Understanding data provenance helps ensure transparency, trustworthiness, and reliability in data usage. </p> <p>Key Questions</p> <ul> <li>What is the source of the data (database, publication, direct experiment)? </li> <li>If data are in classes, how many data points are available in each class\u2014for example, total for the positive (Npos) and negative (Nneg) cases? </li> <li>If regression, how many real value points are there? </li> <li>Has the dataset been previously used by other papers and/or is it recognized by the community?</li> </ul> <p>From Example Publication</p> <p>Protein Data Bank (PDB). X-ray structures missing residues.  Npos = 339,603 residues.  Nneg = 6,168,717 residues.  Previously used in (Walsh et al., Bioinformatics 2015) as an independent benchmark set.</p>"},{"location":"chapters/chapter_01/#12-dataset-splits","title":"1.2 Dataset Splits","text":"<p> Dataset splits refer to the process of dividing a dataset into distinct subsets for different purposes, mainly in machine learning or data science tasks. The most common splits are: </p> <ul> <li> <p>Training Set: This is the largest subset, used to train the machine learning model. The model \u201clearns\u201d from this data by adjusting its internal parameters to minimize prediction errors.</p> </li> <li> <p>Validation Set: A separate subset used to fine-tune the model\u2019s hyperparameters. The model doesn\u2019t learn directly from this data, but it helps monitor the model\u2019s performance and avoid overfitting, which is when a model becomes too tailored to the training data and doesn\u2019t generalize well.</p> </li> <li> <p>Test Set: This is the final subset, used to evaluate the model\u2019s performance. The test set remains unseen by the model until after training and validation are complete, providing an unbiased estimate of how well the model generalizes to new, unseen data.</p> </li> </ul> <p> In addition to these, there are some variations in dataset splitting strategies: </p> <ul> <li> <p>Holdout Split: A simple division where a fixed percentage of data is reserved for testing (e.g., 80% training, 20% test).</p> </li> <li> <p>Cross-validation: In this technique, the dataset is split multiple times into training and validation sets, ensuring each data point is used for validation at least once (e.g., 5-fold cross-validation). This provides a more robust evaluation of the model\u2019s performance.</p> </li> </ul> <p>Key Questions</p> <ul> <li>How many data points are in the training and test sets? </li> <li>Was a separate validation set used, and if yes, how large was it? </li> <li>Are the distributions of data types (N<sub>pos</sub> and N<sub>neg</sub>) in the training and test sets different? Are the distributions of data types in both training and test sets plotted?</li> </ul> <p>From Example Publication</p> <p>Protein Data Bank (PDB). X-ray structures missing residues.  Npos = 339,603 residues.  Nneg = 6,168,717 residues.  Previously used in (Walsh et al., Bioinformatics 2015) as an independent benchmark set.</p>"},{"location":"chapters/chapter_01/#13-redundancy-between-data-splits","title":"1.3 Redundancy between data splits","text":"<p>  Redundancy between data splits occurs when the same data points are present in more than one of the training, validation, or test sets. This is undesirable because it can distort model evaluation and lead to overoptimistic performance metrics (e.g. eliminating data points more similar than X%).  This may effect the mfodel by introcuding an overfitting risk, unreliable performance metrics and/or lack of generalization. </p> <p>Key Questions</p> <ul> <li>How were the sets split? </li> <li>Are the training and test sets independent? </li> <li>How was this enforced (for example, redundancy reduction to less than X% pairwise identity)? </li> <li>How does the distribution compare to previously published ML datasets?</li> </ul> <p>From Example Publication</p> <p>Not applicable.  </p>"},{"location":"chapters/chapter_01/#14-availability-of-data","title":"1.4 Availability of data","text":"<p> Availability of data refers to the accessibility and readiness of data for use in various applications, such as analysis, machine learning, decision-making, or reporting. It ensures that data can be retrieved and utilized when needed by users or systems. </p> <p>Key Questions</p> <ul> <li>Are the data, including the data splits used, released in a public forum? </li> <li>If yes, where (for example, supporting material, URL) and how (license)?</li> </ul> <p>From Example Publication</p> <p>Yes, URL: http://protein.bio.unipd.it/mobidblite/.  Free use license.</p> <p> </p> <ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/chapter_02/","title":"2. Optimisation","text":"<p> Optimization, or model training, refers to the process of adjusting the values that make up the model (including both parameters and hyperparameters) to enhance the model's performance in solving a given problem. In this section, we will focus on challenges that arise from selecting suboptimal optimization strategies. </p> Further Reading <p>Optimization, or training, involves adjusting the values that define a model (such as parameters and hyperparameters), as well as preprocessing steps, to enhance the model\u2019s ability to solve a given problem. Choosing an inappropriate optimization strategy can lead to issues like overfitting or underfitting.</p> <p>Overfitting occurs when a model performs exceptionally well on training data but fails on unseen data, making it ineffective in real-world scenarios. Underfitting, on the other hand, happens when overly simplistic models, capable of capturing only basic relationships between features, are applied to more complex data.</p> <p>Feature selection algorithms can help reduce the risk of overfitting, but they come with their own set of guidelines. A key recommendation is to avoid using non-training data for feature selection and preprocessing. This is especially problematic for meta-predictors, as it can lead to an overestimation of the model\u2019s performance.</p> <p>Finally, making files that detail the exact optimization protocol, including parameters and hyperparameters, publicly available is crucial. A lack of proper documentation and limited access to these records can hinder the understanding and evaluation of the model\u2019s overall performance.<sup>1</sup></p>"},{"location":"chapters/chapter_02/#21-algorithm","title":"2.1 Algorithm","text":"<p> Since algorithms take input data and produce output, typically solving a particular problem or achieving a specific objective, it is essential to know which one is implemented in a study. In this way we can have better insights for the results of learning patterns, relationships, or rules that can then be applied to new, unseen data. Regarding ML class there are three major categories: </p> <ul> <li>Supervised (i.e. Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines (SVM) and others), </li> <li>Unsupervised Learning (i.e. K-Means Clustering, Principal Component Analysis (PCA) and Hierarchical Clustering and others),  </li> <li>Reinforcement Learning (i.e. Q-Learning, Deep Q-Networks (DQN) and others). </li> </ul> <p>Key Questions</p> <ul> <li>What is the ML algorithm class used? </li> <li>Is the ML algorithm new? </li> <li>If yes, why was it chosen over better known alternatives?</li> </ul> <p>From Example Publication</p> <p>Majority-based consensus classification based on 8 primary ML methods and post-processing.</p>"},{"location":"chapters/chapter_02/#22-meta-predictions","title":"2.2 Meta-predictions","text":"<p> Meta-predictions refer to predictions made by models that aggregate or utilize the outputs (predictions) of other models. Essentially, meta-prediction systems combine predictions from multiple models to produce a more robust or accurate final prediction. Meta-predictions are often used in ensemble learning techniques, where the goal is to leverage the strengths of several models to enhance overall performance. </p> <p>Key Questions</p> <ul> <li>Does the model use data from other ML algorithms as input? </li> <li>If yes, which ones? </li> <li>Is it clear that training data of initial predictors and meta-predictor are independent of test data for the meta-predictor?</li> </ul> <p>From Example Publication</p> <p>Yes, predictor output is a binary prediction computed from the consensus of other methods; Independence of training sets of other methods with test set of meta-predictor was not tested since datasets from other methods were not available.</p>"},{"location":"chapters/chapter_02/#23-data-encoding","title":"2.3 Data encoding","text":"<p> Data encoding is the process of transforming data from one format or structure into another, often to make it easier for ML models or computational systems to process.  In ML, data often needs to be encoded to ensure that it can be effectively interpreted by algorithms, especially for algorithms that require numerical input (e.g., neural networks, SVMs). </p> <p>Key Questions</p> <ul> <li>How were the data encoded and preprocessed for the ML algorithm?</li> </ul> <p>From Example Publication</p> <p>Label-wise average of 8 binary predictions.</p>"},{"location":"chapters/chapter_02/#24-parameters","title":"2.4 Parameters","text":"<p> Model parameters are the internal configurations or variables that a model learns from the training data.  These parameters determine how the model makes predictions and how well it fits the training data.  The values of these parameters are adjusted during the training process through algorithms like gradient descent or optimization procedures.  </p> <p>Key Questions</p> <ul> <li>How many parameters (p) are used in the model? </li> <li>How were p selected?</li> </ul> <p>From Example Publication</p> <p>p = 3 (Consensus score threshold, expansion-erosion window, length threshold).  No optimization.</p>"},{"location":"chapters/chapter_02/#25-features","title":"2.5 Features","text":"<p> In the context of ML, features refer to the individual measurable properties or characteristics of the data being used for training a model.  They play a crucial role in determining the performance of ML models, as they provide the information that the model needs to make predictions or classifications. Feature Engineering is the process of creating, modifying, or selecting the most relevant features from the raw data to improve model performance by reducing model complexity, improving training time and avoiding overfitting.  </p> <p>Key Questions</p> <ul> <li>How many features (f) are used as input? </li> <li>Was feature selection performed? </li> <li>If yes, was it performed using the training set only?</li> </ul> <p>From Example Publication</p> <p>Not applicable.</p>"},{"location":"chapters/chapter_02/#26-fitting","title":"2.6 Fitting","text":"<p> Fitting refers to the process of training a ML model on a dataset by adjusting its parameters to minimize prediction error.  The goal is to find a balance between underfitting and overfitting, ensuring that the model captures the underlying patterns in the data while still generalizing well to unseen data.  Proper evaluation, regularization, and tuning of the model during the fitting process are crucial to achieving a good fit. </p> <p>Key Questions</p> <ul> <li>Is p much larger than the number of training points and/or is f large (for example, in classification is p &gt;&gt; (N<sub>pos</sub> + N<sub>neg</sub>) and/or f &gt; 100)? </li> <li>If yes, how was overfitting ruled out? </li> <li>Conversely, if the number of training points is much larger than p and/or f is small (for example, (N<sub>pos</sub> + N<sub>neg</sub>) &gt;&gt; p  and/or f &lt; 5), how was underfitting ruled out?</li> </ul> <p>From Example Publication</p> <p>Single input ML methods are used with default parameters.  Optimization is a simple majority.</p>"},{"location":"chapters/chapter_02/#27-regularization","title":"2.7 Regularization","text":"<p> Regularization is a technique used to prevent overfitting by adding a penalty to the loss function, which discourages the model from becoming too complex. Common regularization techniques include: </p> <ul> <li>L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of the coefficients. It encourages sparsity, setting some coefficients to zero.</li> <li>L2 Regularization (Ridge): Adds a penalty proportional to the square of the coefficients, discouraging large coefficients and thus reducing model complexity.</li> <li>Dropout (in neural networks): Randomly drops a percentage of neurons during training, which helps prevent overfitting by forcing the network to generalize.</li> </ul> <p>Key Questions</p> <ul> <li> <p>Were any overfitting prevention techniques used (for example, early stopping using a validation set)?</p> </li> <li> <p>If yes, which ones?</p> </li> </ul> <p>From Example Publication</p> <p>No. </p>"},{"location":"chapters/chapter_02/#28-availability-of-configuration","title":"2.8 Availability of configuration","text":"<p>  Availability of configuration refers to the accessibility and transparency of the settings, parameters, and options that can be adjusted or customized in a ML model or system.  These configurations control how the model is trained, how it makes predictions, and how it operates in different environments.  Ensuring that the configuration is available, flexible, and easy to modify is important for reproducibility, fine-tuning, and deployment of models. </p> <p>Key Questions</p> <ul> <li>Are the hyperparameter configurations, optimization schedule, model files and optimization parameters reported? </li> <li>If yes, where (for example, URL) and how (license)?</li> </ul> <p>From Example Publication</p> <p>Not applicable.</p> <p> </p> <ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/chapter_03/","title":"3. Model","text":"<p> Good overall performance and the model's ability to generalize well to unseen data are crucial factors that significantly impact the applicability of any proposed ML research. However, several other important aspects related to ML models must also be considered. </p> Further Reading <p>Equally important aspects of ML models include their interpretability and reproducibility. Interpretable models can identify causal relationships in the data and provide logical explanations for their predictions, which is especially valuable in fields like drug design and diagnostics. In contrast, black box models, while often accurate, may not offer understandable insights into the reasons behind their predictions. Both types of models are discussed in more detail elsewhere, and choosing between them involves weighing their respective benefits. The key recommendation is to clearly state whether the model is a black box or interpretable, and if it is interpretable, to provide clear examples of its outputs.</p> <p>Reproducibility is crucial for ensuring that research outcomes can be effectively utilized and validated by the broader community. Challenges with model reproducibility go beyond merely documenting parameters, hyperparameters, and optimization protocols. Limited access to essential model components (such as source code, model files, parameter configurations, and executables) and high computational demands for running trained models on new data can severely restrict or even prevent reproducibility.<sup>1</sup></p>"},{"location":"chapters/chapter_03/#31-interpretability","title":"3.1 Interpretability","text":"<p> Model interpretability refers to the extent to which a human can understand the decisions and predictions made by a ML model.  Interpretability is crucial for building trust in model outcomes, especially in high-stakes domains such as healthcare and finance, where understanding the rationale behind a model's predictions can have significant implications. For example neural networks are often criticized for being \"black boxes,\" as their internal workings (like hidden layers) are less transparent, making them more difficult to trust. There are generally two types of interpretability: </p> <ul> <li> <p></p><p> Global Interpretability refers to the ability to understand the overall behavior of the model across all predictions. It involves explaining how the model works as a whole and what features are important in determining predictions. </p> </li> <li> <p></p><p> Local interpretability focuses on understanding individual predictions made by the model. It aims to explain why a specific input led to a particular output. </p> </li> </ul> <p>Key Questions</p> <ul> <li>Is the model black box or interpretable? </li> <li>If the model is interpretable, can you give clear examples of this?</li> </ul> <p>From Example Publication</p> <p>Transparent, in so far as meta-prediction is concerned. Consensus and post processing over other methods predictions (which are mostly black boxes). No attempt was made to make the meta-prediction a black box.</p>"},{"location":"chapters/chapter_03/#32-output","title":"3.2 Output","text":"<p> The output of a machine learning model refers to the predictions or classifications made by the model after processing input data.  Depending on the type of model and the nature of the problem, the output can vary widely.  Here\u2019s a breakdown of some different types of outputs:  </p> <ul> <li> <p></p><p> Regression includes continuous values that estimates a quantity based on the input features </p> </li> <li> <p></p><p> In classification tasks the output is a category or class label that indicates which class the input belongs to. </p> </li> <li> <p></p><p> In multi-class classification the model predicts one class from multiple possible classes. </p> </li> <li> <p></p><p> Multi-label classification includes the assignment of multiple classes to a single input.</p> </li> </ul> <p>Key Questions</p> <ul> <li>Is the model classification or regression?</li> </ul> <p>From Example Publication</p> <p>Classification, i.e. residues thought to be disordered.</p>"},{"location":"chapters/chapter_03/#33-execution-time","title":"3.3 Execution time","text":"<p> Execution time in the context of ML refers to the duration it takes for a model to perform a specific task, such as training, predicting, or evaluating performance.  Understanding and measuring execution time is crucial for various reasons, including optimizing model performance, resource management, and user experience.  </p> <p>CPU time of single representative execution on standard hardware (e.g. seconds on desktop PC).</p> <p>Key Questions</p> <ul> <li>How much time does a single representative prediction require on a standard machine (for example, seconds on a desktop PC or high-performance computing cluster)?</li> </ul> <p>From Example Publication</p> <p>ca. 1 second per representative on a desktop PC.</p>"},{"location":"chapters/chapter_03/#34-availability-of-software","title":"3.4 Availability of software","text":"<p> Availability of software refers to the accessibility, reliability, and usability of various software tools and libraries that facilitate the development, training, deployment, and evaluation of ML models.  This includes both open-source and proprietary software, and it is critical for researchers and practitioners to have the right tools at their disposal to effectively work on tasks. </p> <p>Key Questions</p> <ul> <li>Is the source code released? </li> <li>Is a method to run the algorithm (executable, web server, virtual machine or container instance) released? </li> <li>If yes, where (for example github, zenodo or other repository URL) and how (for example MIT license)?</li> </ul> <p>From Example Publication</p> <p>Yes, URL: http://protein.bio.unipd.it/mobidblite/.   Bespoke license free for academic use</p> <p> </p> <ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/chapter_04/","title":"4. Evaluation","text":"<p> In implementing a robust and trustworthy ML method, providing a comprehensive data description, adhering to a correct optimization protocol, and ensuring that the model is clearly defined and openly accessible are critical first steps. Equally important is employing a valid assessment methodology to evaluate the final model. </p> Further Reading <p>In biological research, there are two main types of evaluation scenarios for ML models: </p> <ol> <li> <p>Experimental Validation: This involves validating the predictions made by the ML model through laboratory experiments. Although highly desirable, this approach is often beyond the scope of many ML studies.</p> </li> <li> <p>Computational Assessment: This involves evaluating the model\u2019s performance using established metrics. This section focuses on computational assessment and highlights a few potential risks.</p> </li> </ol> <p>When it comes to performance metrics, which are quantifiable indicators of a model\u2019s ability to address a specific task, there are numerous metrics available for various ML classification and regression problems. The wide range of options, along with the domain-specific knowledge needed to choose the right metrics, can result in the selection of inappropriate performance measures. It is advisable to use metrics recommended by critical assessment communities relevant to biological ML models, such as the Critical Assessment of Protein Function Annotation (CAFA) and the Critical Assessment of Genome Interpretation (CAGI).</p> <p>Once appropriate performance metrics are selected, methods published in the same biological domain should be compared using suitable statistical tests (e.g., Student\u2019s t-test) and confidence intervals. Additionally, to avoid releasing ML methods that seem advanced but do not outperform simpler algorithms, it is important to compare these methods against baseline models and demonstrate their statistical superiority (e.g., comparing shallow versus deep neural networks).<sup>1</sup></p>"},{"location":"chapters/chapter_04/#41-evaluation-method","title":"4.1 Evaluation method","text":"<p> Evaluation of a ML model is the process of assessing its performance and effectiveness in making predictions or classifications based on new, unseen data.  Proper evaluation is crucial to ensure that the model generalizes well and performs as expected in real-world applications. </p> <p>Key Questions</p> <ul> <li>How was the method evaluated (for example cross-validation, independent dataset, novel experiments)?</li> </ul> <p>From Example Publication</p> <p>Independent dataset</p>"},{"location":"chapters/chapter_04/#42-performance-measures","title":"4.2 Performance measures","text":"<p> The choice of evaluation metrics depends on the type of problem (regression or classification) and the specific goals of the analysis. </p> Regression Metrics Classification Metrics Mean Absolute Error (MAE) Accuracy Mean Squared Error (MSE) Precision Root Mean Squared Error (RMSE) Recall (Sensitivity) R-squared (R<sup>2</sup>) F1 Score <p>Key Questions</p> <ul> <li>Which performance metrics are reported (Accuracy, sensitivity, specificity, etc.)? </li> <li>Is this set representative (for example, compared to the literature)?</li> </ul> <p>From Example Publication</p> <p>Balanced Accuracy, Precision, Sensitivity, Specificity, F1, MCC.</p>"},{"location":"chapters/chapter_04/#43-comparison","title":"4.3 Comparison","text":"<p> Comparison typically refers to the evaluation of different models, algorithms, or configurations to identify which one performs best for a specific task.  This process is essential for selecting the most suitable approach for a given problem, optimizing performance, and understanding the strengths and weaknesses of various methods.  </p> <p>Key Questions</p> <ul> <li>Was a comparison to publicly available methods performed on benchmark datasets? </li> <li>Was a comparison to simpler baselines performed?</li> </ul> <p>From Example Publication</p> <p>DisEmbl-465, DisEmbl-HL, ESpritz Disprot, ESpritz NMR, ESpritz Xray, Globplot, IUPred long, IUPred short, VSL2b. Chosen methods are the methods from which the meta prediction is obtained.</p>"},{"location":"chapters/chapter_04/#44-confidence","title":"4.4 Confidence","text":"<p> Confidence in the context of ML refers to the measure of certainty or belief that a model's prediction is accurate.  It quantifies the model's certainty regarding its output, which is particularly important in classification tasks, where decisions need to be made based on predicted class probabilities. This can be supported with medthods such as confidence intervals and statistical significance. </p> <p>Key Questions</p> <ul> <li>Do the performance metrics have confidence intervals? </li> <li>Are the results statistically significant to claim that the method is superior to others and baselines?</li> </ul> <p>From Example Publication</p> <p>Not calculated.</p>"},{"location":"chapters/chapter_04/#45-availability-of-evaluation","title":"4.5 Availability of evaluation","text":"<p> Availability of evaluation in ML refers to the accessibility and readiness of tools, frameworks, datasets, and methodologies used to assess the performance of ML models.  This encompasses various aspects, from the datasets used for evaluation to the metrics and software tools that facilitate the evaluation process.  </p> <p>Key Questions</p> <ul> <li>Are the raw evaluation files (for example, assignments for comparison and baselines, statistical code, confusion matrices) available? </li> <li>If yes, where (for example, URL) and how (license)?</li> </ul> <p>From Example Publication</p> <p>Not.</p> <p> </p> <ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/introduction/","title":"Introduction","text":""},{"location":"chapters/introduction/#the-need-for-standardization","title":"The need for standardization","text":"<p>With the significant drop in the cost of many high-throughput technologies, vast amounts of biological data are being generated and made available to researchers.  Machine learning (ML) has emerged as a powerful tool for analyzing data related to cellular processes, genomics, proteomics, post-translational modifications, metabolism, and drug discovery, offering the potential for transformative medical advancements.  This trend is evident in the growing number of ML publications, showcasing a wide array of modeling techniques in biology.  However, although ML methods should ideally be experimentally validated, this occurs in only a small portion of the studies.  We believe the time is right for the ML community to establish standards for reporting ML-based analyses to facilitate critical evaluation and enhance reproducibility.<sup>1</sup></p> <p>The number of ML publications per year is based on Web of Science from 1996 onwards using the topic category for \u201cmachine learning\u201d in combination with each of the following terms: \u201cbiolog*\u201d, \u201cmedicine\u201d, \u201cgenom*\u201d, \u201cprote*\u201d, \u201ccell*\u201d, \u201cpost translational\u201d, \u201cmetabolic\u201d and \u201cclinical\u201d.</p> <p>Guidelines or recommendations on the proper construction of machine learning (ML) algorithms can help ensure accurate results and predictions.  In biomedical research, various communities have established standard guidelines and best practices for managing scientific data and ensuring the reproducibility of computational tools.  Similarly, within the ML community, there is a growing need for a unified set of recommendations that address data handling, optimization techniques, model development, and evaluation protocols comprehensively.</p> <p>A recent commentary emphasized the need for standards in ML,suggesting that introducing submission checklists could be a first step toward improving publication practices.  In response,  a community-driven consensus list of minimal requirements was proposed, framed as questions for ML implementers. By adhering to these guidelines, the quality and reliability of reported methods can be more accurately assessed.  Our focus is on data, optimization, model, and evaluation (DOME), as these four components encompass the core aspects of most ML implementations. These recommendations are primarily aimed at supervised learning in biological applications where direct experimental validation is absent, as this is the most commonly used ML approach.  We do not address the use of ML in clinical settings, and it remains to be seen whether the DOME recommendations can be applied to other areas of ML, such as unsupervised, semi-supervised, or reinforcement learning.</p>"},{"location":"chapters/introduction/#development-of-the-recommendations","title":"Development of the recommendations","text":"Broad topic Be on the lookout for Consequences Recommendation(s) Data \u2022 Inadequate data size &amp; quality  \u2022 Inappropriate partitioning, dependence between train and test data  \u2022 Class imbalance  \u2022 No access to data  \u2022 Data not representative of domain application  \u2022 Unreliable or biased performance evaluation  \u2022 Cannot check data credibility \u2022 Use independent optimization (training) and evaluation (testing) sets. This is especially important for meta algorithms, where independence of multiple training sets must be shown to be independent of the evaluation (testing) sets.  \u2022 Release data, preferably using appropriate long-term repositories, and include exact splits.  \u2022 Offer sufficient evidence of data size &amp; distribution being representative of the domain. Optimization \u2022 Overfitting, underfitting, and illegal parameter tuning  \u2022 Imprecise parameters and protocols given  \u2022 Reported performance is too optimistic or too pessimistic  \u2022 The model models noise or misses relevant relationships  \u2022 Results are not reproducible \u2022 Clarify that evaluation sets were not used for feature selection. \u2022 Report indicators on training and testing data that can aid in assessing the possibility of under- or overfitting; for example, train vs. test error. \u2022 Release definitions of all algorithmic hyperparameters, regularization protocols, parameters and optimization protocol.  \u2022 For neural networks, release definitions of training and learning curves.    \u2022 Include explicit model validation techniques like N-fold cross-validation. Model \u2022 Unclear if black box or interpretable model  \u2022 No access to resulting source code, trained models &amp; data  \u2022 Execution time impractical \u2022 An interpretable model shows no explainable behavior  \u2022 Cannot cross compare methods &amp; reproducibility, or check data credibility  \u2022 Model takes too much time to produce results \u2022 Describe the choice of black box or interpretable model. If interpretable, show examples of interpretable output.  \u2022 Release documented source code + models + software containers.  \u2022 Report execution time averaged across repeats. If computationally tough, compare to similar methods. Evaluation \u2022 Performance measures inadequate  \u2022 No comparisons to baselines or other methods  \u2022 Highly variable performance \u2022 Biased performance measures reported  \u2022 The method is falsely claimed as state-of-the-art  \u2022 Unpredictable performance in production \u2022 Compare with public methods &amp; simple models (baselines). \u2022 Adopt community-validated measures and benchmark datasets for evaluation.  \u2022 Compare related methods and alternatives on the same dataset.   \u2022 Evaluate performance on a final independent held-out set.   \u2022 Use confidence intervals/error intervals and statistical tests to gauge robustness. <p>The recommendations mentioned above were initially developed by the ELIXIR Machine Learning Focus Group in response to a published Comment advocating for the establishment of standards in ML for biology.  This focus group, comprising over 50 experts in the field of ML, held meetings to collaboratively develop and refine the recommendations through broad consensus.</p> <p>In the following chapters the publication from <sup>2</sup> is going to be used as an example.</p> <ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> <li> <p>Marco Necci, Damiano Piovesan, Zsuzsanna Doszt\u00e1nyi, and Silvio C.E Tosatto. MobiDB-lite: fast and highly specific consensus prediction of intrinsic disorder in proteins. Bioinformatics, 33(9):1402\u20131404, 01 2017. URL: https://doi.org/10.1093/bioinformatics/btx015, arXiv:https://academic.oup.com/bioinformatics/article-pdf/33/9/1402/49038981/bioinformatics\\_33\\_9\\_1402.pdf, doi:10.1093/bioinformatics/btx015.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/references/","title":"References","text":"<ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> <li> <p>Marco Necci, Damiano Piovesan, Zsuzsanna Doszt\u00e1nyi, and Silvio C.E Tosatto. MobiDB-lite: fast and highly specific consensus prediction of intrinsic disorder in proteins. Bioinformatics, 33(9):1402\u20131404, 01 2017. URL: https://doi.org/10.1093/bioinformatics/btx015, arXiv:https://academic.oup.com/bioinformatics/article-pdf/33/9/1402/49038981/bioinformatics\\_33\\_9\\_1402.pdf, doi:10.1093/bioinformatics/btx015.\u00a0\u21a9</p> </li> </ol>"},{"location":"keywords/","title":"Keywords","text":"<p>Here\u2019s a lit of used keywords:</p> <ol> <li> <p>Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, Emidio Capriotti, Rita Casadio, Salvador Capella-Gutierrez, Davide Cirillo, Alessio Del Conte, Alexandros C. Dimopoulos, Victoria Dominguez Del Angel, Joaquin Dopazo, Piero Fariselli, Jos\u00e9 Maria Fern\u00e1ndez, Florian Huber, Anna Kreshuk, Tom Lenaerts, Pier Luigi Martelli, Arcadi Navarro, Pilib \u00d3 Broin, Janet Pi\u00f1ero, Damiano Piovesan, Martin Reczko, Francesco Ronzano, Venkata Satagopam, Castrense Savojardo, Vojtech Spiwok, Marco Antonio Tangaro, Giacomo Tartari, David Salgado, Alfonso Valencia, Federico Zambelli, Jennifer Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto, and ELIXIR Machine Learning Focus Group. Dome: recommendations for supervised machine learning validation in biology. Nature Methods, 18(10):1122\u20131127, 2021. URL: https://doi.org/10.1038/s41592-021-01205-4, doi:10.1038/s41592-021-01205-4.\u00a0\u21a9</p> </li> <li> <p>Marco Necci, Damiano Piovesan, Zsuzsanna Doszt\u00e1nyi, and Silvio C.E Tosatto. MobiDB-lite: fast and highly specific consensus prediction of intrinsic disorder in proteins. Bioinformatics, 33(9):1402\u20131404, 01 2017. URL: https://doi.org/10.1093/bioinformatics/btx015, arXiv:https://academic.oup.com/bioinformatics/article-pdf/33/9/1402/49038981/bioinformatics\\_33\\_9\\_1402.pdf, doi:10.1093/bioinformatics/btx015.mkdocs\u00a0\u21a9</p> </li> </ol>"}]}